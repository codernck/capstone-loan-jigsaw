{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428f26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports at one place here\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9f7a318",
   "metadata": {},
   "source": [
    "# Loading the Data from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8897828",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = \"<data-url>\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "infer_schema = \"true\"\n",
    "first_row_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark_df = spark.read.format(file_type) \\\n",
    "     .option(\"inferSchema\", infer_schema) \\\n",
    "     .option(\"header\", first_row_header) \\\n",
    "     .option(\"sep\", delimiter) \\\n",
    "     .load(file_loc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d9b7198",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a866a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- disbursed_amount: integer (nullable = true)\n",
      " |-- asset_cost: integer (nullable = true)\n",
      " |-- ltv: double (nullable = true)\n",
      " |-- branch_id: integer (nullable = true)\n",
      " |-- Date.of.Birth: string (nullable = true)\n",
      " |-- Employment.Type: string (nullable = true)\n",
      " |-- DisbursalDate: string (nullable = true)\n",
      " |-- MobileNo_Avl_Flag: integer (nullable = true)\n",
      " |-- Aadhar_flag: integer (nullable = true)\n",
      " |-- PAN_flag: integer (nullable = true)\n",
      " |-- VoterID_flag: integer (nullable = true)\n",
      " |-- Driving_flag: integer (nullable = true)\n",
      " |-- Passport_flag: integer (nullable = true)\n",
      " |-- PERFORM_CNS.SCORE: integer (nullable = true)\n",
      " |-- DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS: integer (nullable = true)\n",
      " |-- CREDIT.HISTORY.LENGTH: string (nullable = true)\n",
      " |-- NO.OF_INQUIRIES: integer (nullable = true)\n",
      " |-- loan_default: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "513d692d",
   "metadata": {},
   "source": [
    "# Cleansing the columns, refactoring the columns which have dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de01a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- disbursed_amount: integer (nullable = true)\n",
      " |-- asset_cost: integer (nullable = true)\n",
      " |-- ltv: double (nullable = true)\n",
      " |-- branch_id: integer (nullable = true)\n",
      " |-- Date_Of_Birth: string (nullable = true)\n",
      " |-- Employment_Type: string (nullable = true)\n",
      " |-- DisbursalDate: string (nullable = true)\n",
      " |-- MobileNo_Avl_Flag: integer (nullable = true)\n",
      " |-- Aadhar_flag: integer (nullable = true)\n",
      " |-- PAN_flag: integer (nullable = true)\n",
      " |-- VoterID_flag: integer (nullable = true)\n",
      " |-- Driving_flag: integer (nullable = true)\n",
      " |-- Passport_flag: integer (nullable = true)\n",
      " |-- PERFORM_CNS_CORE: integer (nullable = true)\n",
      " |-- DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS: integer (nullable = true)\n",
      " |-- CREDIT_HISTORY_LENGTH: string (nullable = true)\n",
      " |-- NO_OF_INQUIRIES: integer (nullable = true)\n",
      " |-- loan_default: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace dots in all the columns with _\n",
    "spark_df1 = spark_df.withColumnRenamed(\"Date.of.Birth\",\"Date_Of_Birth\") \\\n",
    ".withColumnRenamed(\"Employment.Type\",\"Employment_Type\")  \\\n",
    ".withColumnRenamed(\"PERFORM_CNS.SCORE\",\"PERFORM_CNS_CORE\") \\\n",
    ".withColumnRenamed(\"DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS\", \"DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS\") \\\n",
    ".withColumnRenamed(\"CREDIT.HISTORY.LENGTH\", \"CREDIT_HISTORY_LENGTH\") \\\n",
    ".withColumnRenamed(\"NO.OF_INQUIRIES\", \"NO_OF_INQUIRIES\") \n",
    "\n",
    "spark_df1.printSchema()\n",
    "\n",
    "spark_df = spark_df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c6fbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[disbursed_amount: int, asset_cost: int, ltv: double, branch_id: int, Date_Of_Birth: string, Employment_Type: string, DisbursalDate: string, MobileNo_Avl_Flag: int, Aadhar_flag: int, PAN_flag: int, VoterID_flag: int, Driving_flag: int, Passport_flag: int, PERFORM_CNS_CORE: int, DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS: int, CREDIT_HISTORY_LENGTH: string, NO_OF_INQUIRIES: int, loan_default: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d69c5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23315"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b37edd42",
   "metadata": {},
   "source": [
    "# Checking for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "271d9f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+---+---------+-------------+---------------+-------------+-----------------+-----------+--------+------------+------------+-------------+----------------+-----------------------------------+---------------------+---------------+------------+-----+\n",
      "|disbursed_amount|asset_cost|ltv|branch_id|Date_Of_Birth|Employment_Type|DisbursalDate|MobileNo_Avl_Flag|Aadhar_flag|PAN_flag|VoterID_flag|Driving_flag|Passport_flag|PERFORM_CNS_CORE|DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS|CREDIT_HISTORY_LENGTH|NO_OF_INQUIRIES|loan_default|count|\n",
      "+----------------+----------+---+---------+-------------+---------------+-------------+-----------------+-----------+--------+------------+------------+-------------+----------------+-----------------------------------+---------------------+---------------+------------+-----+\n",
      "+----------------+----------+---+---------+-------------+---------------+-------------+-----------------+-----------+--------+------------+------------+-------------+----------------+-----------------------------------+---------------------+---------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_duplicates = spark_df.groupBy(spark_df.columns).count().filter(\"count > 1\")\n",
    "df_duplicates.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49881cd9",
   "metadata": {},
   "source": [
    "#### There are no duplicates as we can see the dataframe returned is empty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8615ed7f",
   "metadata": {},
   "source": [
    "# Checking for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7517b0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'disbursed_amount': 0,\n",
       " 'asset_cost': 0,\n",
       " 'ltv': 0,\n",
       " 'branch_id': 0,\n",
       " 'Date_Of_Birth': 0,\n",
       " 'Employment_Type': 770,\n",
       " 'DisbursalDate': 0,\n",
       " 'MobileNo_Avl_Flag': 0,\n",
       " 'Aadhar_flag': 0,\n",
       " 'PAN_flag': 0,\n",
       " 'VoterID_flag': 0,\n",
       " 'Driving_flag': 0,\n",
       " 'Passport_flag': 0,\n",
       " 'PERFORM_CNS_CORE': 0,\n",
       " 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS': 0,\n",
       " 'CREDIT_HISTORY_LENGTH': 0,\n",
       " 'NO_OF_INQUIRIES': 0,\n",
       " 'loan_default': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking null/na in all the columns\n",
    "nulls_df = {col:spark_df.filter(spark_df[col].isNull()).count() for col in spark_df.columns}\n",
    "nulls_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bc792e1",
   "metadata": {},
   "source": [
    "#### We can observe the Employment_Type column has 770 nulls, we will now treat the null values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b739fbe7",
   "metadata": {},
   "source": [
    "## Treating the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbf5152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|Employment_Type|count|\n",
      "+---------------+-----+\n",
      "|           null|  770|\n",
      "|  Self employed|12724|\n",
      "|       Salaried| 9821|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupBy('Employment_Type').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0e3eeaa",
   "metadata": {},
   "source": [
    "#### Self employed is having high count compared to others so we will replace the null with Self employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b0ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.fillna(\"Self employed\",[\"Employment_Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "278e5614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|Employment_Type|count|\n",
      "+---------------+-----+\n",
      "|  Self employed|13494|\n",
      "|       Salaried| 9821|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validation\n",
    "spark_df.groupBy('Employment_Type').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa96d383",
   "metadata": {},
   "source": [
    "# Cleansing the Date Of Birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8977a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Date_Of_Birth|\n",
      "+-------------+\n",
      "|     14-06-90|\n",
      "|     1/1/1991|\n",
      "|     16-08-93|\n",
      "|     1/1/1989|\n",
      "|     31-12-74|\n",
      "|     23-11-64|\n",
      "|    1/10/1989|\n",
      "|     1/1/1995|\n",
      "|     15-06-94|\n",
      "|     23-11-82|\n",
      "|     1/1/1984|\n",
      "|     15-02-90|\n",
      "|    5/12/1975|\n",
      "|    11/9/1995|\n",
      "|     29-10-75|\n",
      "|     28-02-84|\n",
      "|    2/11/1975|\n",
      "|     21-10-90|\n",
      "|   11/10/1994|\n",
      "|     5/6/1987|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"Date_Of_Birth\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b5ca520",
   "metadata": {},
   "source": [
    "#### The date of birth has two formats of dates the dd-MM-yy is not giving proper date with defualt to_date() function we we will be using the spark.sql.legacy.timeParserPolicy to handle that dd-MM-yy format "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9c1e57c",
   "metadata": {},
   "source": [
    "#### Note: Tried with spark.sql.legacy.timeParserPolicy but it was not working, so finally applied regex on the date for cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "092dbd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark_df.withColumn(\n",
    "    'Date_Of_Birth',\n",
    "    when(\n",
    "        spark_df[\"Date_Of_Birth\"].rlike(r'^.*-[^/]+$') & spark_df[\"Date_Of_Birth\"].rlike(r'^.*-\\d{2}$') & (spark_df[\"Date_Of_Birth\"].substr(-2, 2) != '00'),\n",
    "        regexp_replace(spark_df[\"Date_Of_Birth\"], r'-(\\d{2})$', '/19$1')\n",
    "    ) \n",
    "    .when(\n",
    "        spark_df[\"Date_Of_Birth\"].rlike(r'^.*-[^/]+$') & spark_df[\"Date_Of_Birth\"].rlike(r'^.*-\\d{2}$') & (spark_df[\"Date_Of_Birth\"].substr(-2, 2) == '00'),\n",
    "        regexp_replace(spark_df[\"Date_Of_Birth\"], r'-(\\d{2})$', '/20$1')\n",
    "    )\n",
    "    .otherwise(spark_df[\"Date_Of_Birth\"])    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a86a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn('Date_Of_Birth',regexp_replace(new_df[\"Date_Of_Birth\"], '-', '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a36233",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn(\n",
    "    'Date_Of_Birth',\n",
    "    when(\n",
    "        new_df[\"Date_Of_Birth\"].contains('/'),\n",
    "        when(\n",
    "            split(new_df[\"Date_Of_Birth\"], '/')[0].cast(\"integer\").between(1, 9),\n",
    "            concat(\n",
    "                lpad(split(new_df[\"Date_Of_Birth\"], '/')[0].cast(\"integer\"), 2, '0'),\n",
    "                lit('/'),\n",
    "                split(new_df[\"Date_Of_Birth\"], '/')[1],\n",
    "                lit('/'),\n",
    "                split(new_df[\"Date_Of_Birth\"], '/')[2]\n",
    "            )\n",
    "        )\n",
    "        .otherwise(new_df[\"Date_Of_Birth\"])\n",
    "    )\n",
    "    .otherwise(new_df[\"Date_Of_Birth\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a15b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Date_Of_Birth|\n",
      "+-------------+\n",
      "|   14/06/1990|\n",
      "|    01/1/1991|\n",
      "|   16/08/1993|\n",
      "|    01/1/1989|\n",
      "|   31/12/1974|\n",
      "|   23/11/1964|\n",
      "|   01/10/1989|\n",
      "|    01/1/1995|\n",
      "|   15/06/1994|\n",
      "|   23/11/1982|\n",
      "|    01/1/1984|\n",
      "|   15/02/1990|\n",
      "|   05/12/1975|\n",
      "|    11/9/1995|\n",
      "|   29/10/1975|\n",
      "|   28/02/1984|\n",
      "|   02/11/1975|\n",
      "|   21/10/1990|\n",
      "|   11/10/1994|\n",
      "|    05/6/1987|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(\"Date_Of_Birth\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "522fdc61",
   "metadata": {},
   "source": [
    "#### Similary claning up the DisbursalDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db6c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark_df.withColumn(\n",
    "    'DisbursalDate',when(\n",
    "        spark_df[\"DisbursalDate\"].rlike(r'^.*-[^/]+$') & spark_df[\"DisbursalDate\"].rlike(r'^.*-\\d{2}$'),\n",
    "        regexp_replace(spark_df[\"DisbursalDate\"], r'-(\\d{2})$', '/20$1')\n",
    "    )\n",
    "    .otherwise(spark_df[\"DisbursalDate\"])    \n",
    ")\n",
    "\n",
    "new_df = new_df.withColumn('DisbursalDate',regexp_replace(new_df[\"DisbursalDate\"], '-', '/'))\n",
    "\n",
    "#spark_df1.withColumn(\"Date_Of_Birth\", to_date(\"Date_Of_Birth\", \"dd-MM-yy\"))park_df1.withColumn(\"Date_Of_Birth\", to_date(\"Date_Of_Birth\", \"dd-MM-yy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a960504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|DisbursalDate|\n",
      "+-------------+\n",
      "|   28/09/2018|\n",
      "|    10/9/2018|\n",
      "|   31/08/2018|\n",
      "|   13/10/2018|\n",
      "|   14/09/2018|\n",
      "|   17/08/2018|\n",
      "|   16/08/2018|\n",
      "|   26/08/2018|\n",
      "|   15/10/2018|\n",
      "|   26/10/2018|\n",
      "|     3/9/2018|\n",
      "|   24/08/2018|\n",
      "|   28/08/2018|\n",
      "|   21/09/2018|\n",
      "|   20/08/2018|\n",
      "|   26/10/2018|\n",
      "|   30/09/2018|\n",
      "|   30/09/2018|\n",
      "|   31/10/2018|\n",
      "|   25/10/2018|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select('DisbursalDate').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eea5a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = new_df.withColumn(\"Date_Of_Birth\", to_date(\"Date_Of_Birth\", \"d/M/y\"))\n",
    "# new_df = new_df.withColumn(\"DisbursalDate\", to_date(\"DisbursalDate\", \"dd/MM/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2712de24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EXCEPTION'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.legacy.timeParserPolicy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4715e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa8ad15c",
   "metadata": {},
   "source": [
    "# Converting the CREDIT_HISTORY_LENGTH to months (2yrs 6mon) will become 30 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15a74363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_months(col):\n",
    "    split_col = split(col, \" \")\n",
    "    years = split_col.getItem(0).cast(\"int\")\n",
    "    months = split_col.getItem(1).cast(\"int\")\n",
    "    total_months = years * 12 + months\n",
    "    return total_months\n",
    "\n",
    "new_df = new_df.withColumn(\"CREDIT_HISTORY_LENGTH_CLN\", regexp_replace(\"CREDIT_HISTORY_LENGTH\", \"yrs\", \"\"))\n",
    "new_df = new_df.withColumn(\"CREDIT_HISTORY_LENGTH_CLN\", regexp_replace(\"CREDIT_HISTORY_LENGTH_CLN\", \"mon\", \"\"))\n",
    "\n",
    "new_df = new_df.withColumn(\"CREDIT_HISTORY_LENGTH_CLN\",convert_months(\"CREDIT_HISTORY_LENGTH_CLN\"))\n",
    "\n",
    "new_df = new_df.drop(\"CREDIT_HISTORY_LENGTH\")\n",
    "#spark_df.withColumnRenamed(\"CREDIT_HISTORY_LENGTH_CLN\",\"CREDIT_HISTORY_LENGTH\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ea1c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validating the CREDIT_HISTORY_LENGTH_CLN\n",
    "\n",
    "# spark_df.createOrReplaceTempView(temp_loans_table)\n",
    "# spark_df.select(\"*\").show()\n",
    "# spark_df.select([col for col in spark_df.columns]).show()\n",
    "\n",
    "#new_df.select(\"CREDIT_HISTORY_LENGTH_CLN\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d1b6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.mode('overwrite').parquet(\"<dest-url>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51282e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
